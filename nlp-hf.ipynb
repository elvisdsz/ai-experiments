{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db260e1",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip3 install torch --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f491734",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194633a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f0be1",
   "metadata": {},
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b416506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9177eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f8dd5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "chatbot = pipeline(task=\"text2text-generation\", model=\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac82d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"\n",
    "Who are you?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fad71ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ' I am a man of many trades.  I am an engineer.  What do you do?'}]\n"
     ]
    }
   ],
   "source": [
    "response = chatbot(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c1000",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6426e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e472578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(task=\"translation\", model=\"facebook/nllb-200-distilled-600M\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b940d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Hello. Who are you and what is the meaning of life?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa53b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200\n",
    "text_translated = translator(text, src_lang=\"eng_Latn\", tgt_lang=\"kan_Knda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92d8ac2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'ಹಲೋ. ನೀವು ಯಾರು ಮತ್ತು ಜೀವನದ ಅರ್ಥವೇನು?'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd19d590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2925"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleanup\n",
    "import gc\n",
    "del translator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b1646",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1707de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4026631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(task=\"summarization\", model=\"facebook/bart-large-cnn\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b66897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knives Out - Plot\n",
    "text = \"\"\"The family of wealthy mystery novelist Harlan Thrombey attends his birthday party at his estate. The next morning, Harlan's housekeeper Fran discovers him dead with a slit throat. Police detectives Lieutenant Elliot and Trooper Wagner believe Harlan died by suicide, but private detective Benoit Blanc is anonymously hired to investigate Harlan's death. Blanc learns Harlan had strained relationships with his family members, giving several of them plausible motives for murder.\n",
    "\n",
    "Unbeknownst to Blanc, Harlan's nurse Marta Cabrera believes she injected Harlan with a lethal dose of morphine after mixing up his bedtime medications the night of the party. To protect Marta from being blamed for his death, Harlan instructed Marta to create a false alibi before he slit his own throat: she was to be seen leaving the house, sneak back in through a window, and disguise herself as Harlan in order to make it appear that he was alive after she left for the night. Marta cannot lie without vomiting, so she gives accurate but incomplete answers when questioned. She agrees to assist Blanc's investigation and conceals evidence incriminating her. At the reading of Harlan's will, Marta is bequeathed his entire fortune and property, stunning the Thrombeys. Harlan's grandson Ransom Drysdale helps Marta flee but manipulates her into confessing to him. Ransom offers further assistance in exchange for a portion of Marta's inheritance. Meanwhile, the other Thrombeys unsuccessfully attempt to influence Marta to renounce the inheritance, even threatening to have her undocumented mother deported.\n",
    "\n",
    "Marta receives a blackmail note containing a partial photocopy of Harlan's toxicology report. She and Ransom drive to the medical examiner's office, only to find it burned down. Marta receives an email proposing a meeting with the blackmailer; Blanc and the police spot them, leading to Ransom being arrested. At the meeting, Marta finds that Fran, the blackmailer, has been drugged; she performs CPR on Fran and calls an ambulance. Marta confesses to Blanc, but discovers Ransom has already implicated her. Out of moral obligation, Marta believes she must confess to the Thrombeys, which would invalidate the bequest under the slayer rule.\n",
    "\n",
    "At the mansion, Marta finds Fran's copy of the full toxicology report, which shows Harlan had only trace amounts of morphine in his blood. Blanc reveals his deductions to the police, Marta, and Ransom. Blanc deduces Harlan told Ransom about his will, prompting Ransom to swap Harlan's medicines to cause Marta to kill him unknowingly. However, Marta actually gave Harlan the correct medication, recognizing it by viscosity without reading the label due to her experience as a nurse; she only believed she had poisoned Harlan after reading the label on the bottle with the switched content. When the death was reported as a suicide, Ransom anonymously hired Blanc to entrap Marta. Fran saw Ransom tampering with the crime scene to remove the switched medications, and sent him the blackmail note. After Ransom realized that Marta was not responsible for Harlan's death but believed she was, he forwarded the blackmail letter to Marta and burned down the medical examiner's office to destroy evidence of her innocence. Ransom then overdosed Fran with morphine, intending for Marta to be caught with Fran's corpse.\n",
    "\n",
    "The hospital calls; Marta relays that Fran has survived and will implicate Ransom. Ransom insists he will avoid criminal charges because his attempt to kill Fran failed. Marta then vomits on Ransom, revealing she lied: Fran is dead. Realizing he has confessed to the murder, and that the police officers recorded his confession, Ransom grabs a knife from Harlan's collection and attacks Marta, but the knife is a harmless retractable stage knife, and the police promptly arrest him.\n",
    "\n",
    "Blanc tells Marta he suspected early on that she played a part in Harlan's death, noting a drop of blood on her shoe. He tells Marta her innocence prevailed because she made ethical choices that obstructed Ransom's attempts to incriminate her. As Ransom is taken into custody and the rest of the family is gathered outside in defeat, Marta watches from the balcony of what is now her mansion, sipping from Harlan's coffee mug that reads \"My House, My Rules, My Coffee!!\". \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b1fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Summary:\n",
      "The family of wealthy mystery novelist Harlan Thrombey attends his birthday party at his estate. The next morning, Harlan's housekeeper Fran discovers him dead with a slit throat. Police detectives Lieutenant Elliot and Trooper Wagner believe Harlan died by suicide, but private detective Benoit Blanc is anonymously hired to investigate. Blanc learns Harlan had strained relationships with his family members, giving several of them plausible motives for murder.\n"
     ]
    }
   ],
   "source": [
    "summary = summarizer(text,\n",
    "                     min_length=10,\n",
    "                     max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0afd2225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"The family of wealthy mystery novelist Harlan Thrombey attends his birthday party at his estate. The next morning, Harlan's housekeeper Fran discovers him dead with a slit throat. Police detectives Lieutenant Elliot and Trooper Wagner believe Harlan died by suicide, but private detective Benoit Blanc is anonymously hired to investigate. Blanc learns Harlan had strained relationships with his family members, giving several of them plausible motives for murder.\"}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc347532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pegasus model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e6101dab3f40778a6531a5e46d6f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9714ebcd28749f69efb56c2801974aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f48d2c296fe45cfa0da155852814df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6047ecb169435f9fe98ddf80c476f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33dd8b103e1446eb8bbd29ce75ae3eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f132ab2e6814f989c536789cd638ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1fb8e8a45d41f6bca6f4a99b96b7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai\\AI-experiments\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2359\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai\\AI-experiments\\.venv\\Lib\\site-packages\\transformers\\models\\pegasus\\tokenization_pegasus_fast.py:136\u001b[39m, in \u001b[36mPegasusTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, pad_token, eos_token, unk_token, mask_token, mask_token_sent, additional_special_tokens, offset, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33madded_tokens_decoder\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_token_sent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token_sent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_slow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_slow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai\\AI-experiments\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai\\AI-experiments\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1857\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1856\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m converter_class(transformer_tokenizer).converted()\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mtekken.json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1858\u001b[39m     transformer_tokenizer.original_tokenizer = transformer_tokenizer\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'endswith'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Try a different model better suited for narrative summarization\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Pegasus is specifically trained for abstractive summarization\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading Pegasus model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m summarizer_pegasus = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummarization\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/pegasus-cnn_dailymail\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSummarizing with Pegasus...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m summary_pegasus = summarizer_pegasus(text, min_length=\u001b[32m80\u001b[39m, max_length=\u001b[32m130\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai\\AI-experiments\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1078\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m load_tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1080\u001b[39m         tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai\\AI-experiments\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1073\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m             tokenizer_kwargs = model_kwargs.copy()\n\u001b[32m   1071\u001b[39m             tokenizer_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), tokenizer_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m         tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m load_tokenizer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai\\AI-experiments\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1175\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1172\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai\\AI-experiments\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2113\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2110\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2111\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai\\AI-experiments\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2360\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2359\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mimport_protobuf_decode_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2361\u001b[39m     logger.info(\n\u001b[32m   2362\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2363\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2364\u001b[39m     )\n\u001b[32m   2365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai\\AI-experiments\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:88\u001b[39m, in \u001b[36mimport_protobuf_decode_error\u001b[39m\u001b[34m(error_message)\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DecodeError\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PROTOBUF_IMPORT_ERROR.format(error_message))\n",
      "\u001b[31mImportError\u001b[39m: \n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# The issue is that BART tends to extract the beginning of text\n",
    "# Let's pre-process by removing the first 1-2 sentences and summarizing the rest\n",
    "# This forces the model to learn from other parts of the text\n",
    "\n",
    "sentences = text.split(\". \")\n",
    "# Skip the first 2 sentences and summarize the middle/end\n",
    "text_without_beginning = \". \".join(sentences[2:])\n",
    "\n",
    "print(\"Summarizing without beginning sentences:\")\n",
    "summary_no_start = summarizer(text_without_beginning, min_length=50, max_length=150)\n",
    "print(summary_no_start[0]['summary_text'])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Now let's create a true abstractive summary by summarizing the full text\n",
    "# but with better parameters\n",
    "print(\"Original BART summary (full text):\")\n",
    "summary_original = summarizer(text, min_length=50, max_length=150)\n",
    "print(summary_original[0]['summary_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
